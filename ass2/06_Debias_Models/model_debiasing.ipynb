{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource, BasicProblem, generate_categories\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess the data\n",
    "We are going to work with the [Folktables](https://github.com/socialfoundations/folktables#quick-start-examples) dataset (*you have already worked with it*). I have chosen some variables for you, but you can add more (*if you like to*) - here is the [full list](https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2021.pdf) of variables (some of them do not exist in `ACSDataSource`). \n",
    "\n",
    "Today we are going to debias a regression model using the `SEX` variable. Your model should predict the *Total person's income*  (I've digitized  it in  `target_transform=lambda x: x > 25000`, you can choose another threshold).\n",
    "\n",
    "\n",
    "* If you code is slow - you can subsample data (aka reduce the number of the samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=False)\n",
    "\n",
    "ACSIncomeNew = BasicProblem(\n",
    "    features=[\n",
    "        'AGEP', # include AGE\n",
    "        'COW', # include class of worker\n",
    "        'SCHL', # include school education\n",
    "        'WKHP', # include reported working hours\n",
    "        'SEX', # include sex\n",
    "        # some random, possibly noisy\n",
    "        'JWMNP' # travel time to work\n",
    "    ],\n",
    "    target='PINCP',\n",
    "    target_transform=lambda x: x > 25000,    \n",
    "    group='SEX',\n",
    "    preprocess=adult_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small snippet to get the names of the categorical variables - I convert categoricals into one-hot encoded (*you don't have to, depending on what assumptions you use about the data*). **Don't forget to normalise the continious features (if you plan to use Cross-Validation features should be normalized per fold, aka not in the global table).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaler__AGEP</th>\n",
       "      <th>scaler__WKHP</th>\n",
       "      <th>scaler__JWMNP</th>\n",
       "      <th>remainder__COW_Employee of a private for-profit company or business, or of an individual, for wages, salary, or commissions</th>\n",
       "      <th>remainder__COW_Employee of a private not-for-profit, tax-exempt, or charitable organization</th>\n",
       "      <th>remainder__COW_Federal government employee</th>\n",
       "      <th>remainder__COW_Local government employee (city, county, etc.)</th>\n",
       "      <th>remainder__COW_Self-employed in own incorporated business, professional practice or farm</th>\n",
       "      <th>remainder__COW_Self-employed in own not incorporated business, professional practice, or farm</th>\n",
       "      <th>remainder__COW_State government employee</th>\n",
       "      <th>...</th>\n",
       "      <th>remainder__SCHL_Grade 9</th>\n",
       "      <th>remainder__SCHL_Kindergarten</th>\n",
       "      <th>remainder__SCHL_Master's degree</th>\n",
       "      <th>remainder__SCHL_No schooling completed</th>\n",
       "      <th>remainder__SCHL_Nursery school, preschool</th>\n",
       "      <th>remainder__SCHL_Professional degree beyond a bachelor's degree</th>\n",
       "      <th>remainder__SCHL_Regular high school diploma</th>\n",
       "      <th>remainder__SCHL_Some college, but less than 1 year</th>\n",
       "      <th>remainder__SEX_Female</th>\n",
       "      <th>remainder__SEX_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.855577</td>\n",
       "      <td>0.163863</td>\n",
       "      <td>-1.002953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.460230</td>\n",
       "      <td>-1.372352</td>\n",
       "      <td>-0.606647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.495849</td>\n",
       "      <td>-2.294081</td>\n",
       "      <td>-0.012189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.654027</td>\n",
       "      <td>0.163863</td>\n",
       "      <td>-1.002953</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.661781</td>\n",
       "      <td>-1.525973</td>\n",
       "      <td>-1.002953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaler__AGEP  scaler__WKHP  scaler__JWMNP  \\\n",
       "0     -0.855577      0.163863      -1.002953   \n",
       "1     -1.460230     -1.372352      -0.606647   \n",
       "2      1.495849     -2.294081      -0.012189   \n",
       "3     -0.654027      0.163863      -1.002953   \n",
       "4     -1.661781     -1.525973      -1.002953   \n",
       "\n",
       "   remainder__COW_Employee of a private for-profit company or business, or of an individual, for wages, salary, or commissions  \\\n",
       "0                                                0.0                                                                             \n",
       "1                                                0.0                                                                             \n",
       "2                                                0.0                                                                             \n",
       "3                                                1.0                                                                             \n",
       "4                                                0.0                                                                             \n",
       "\n",
       "   remainder__COW_Employee of a private not-for-profit, tax-exempt, or charitable organization  \\\n",
       "0                                                0.0                                             \n",
       "1                                                0.0                                             \n",
       "2                                                1.0                                             \n",
       "3                                                0.0                                             \n",
       "4                                                1.0                                             \n",
       "\n",
       "   remainder__COW_Federal government employee  \\\n",
       "0                                         0.0   \n",
       "1                                         0.0   \n",
       "2                                         0.0   \n",
       "3                                         0.0   \n",
       "4                                         0.0   \n",
       "\n",
       "   remainder__COW_Local government employee (city, county, etc.)  \\\n",
       "0                                                0.0               \n",
       "1                                                0.0               \n",
       "2                                                0.0               \n",
       "3                                                0.0               \n",
       "4                                                0.0               \n",
       "\n",
       "   remainder__COW_Self-employed in own incorporated business, professional practice or farm  \\\n",
       "0                                                0.0                                          \n",
       "1                                                0.0                                          \n",
       "2                                                0.0                                          \n",
       "3                                                0.0                                          \n",
       "4                                                0.0                                          \n",
       "\n",
       "   remainder__COW_Self-employed in own not incorporated business, professional practice, or farm  \\\n",
       "0                                                1.0                                               \n",
       "1                                                0.0                                               \n",
       "2                                                0.0                                               \n",
       "3                                                0.0                                               \n",
       "4                                                0.0                                               \n",
       "\n",
       "   remainder__COW_State government employee  ...  remainder__SCHL_Grade 9  \\\n",
       "0                                       0.0  ...                      0.0   \n",
       "1                                       1.0  ...                      0.0   \n",
       "2                                       0.0  ...                      0.0   \n",
       "3                                       0.0  ...                      0.0   \n",
       "4                                       0.0  ...                      0.0   \n",
       "\n",
       "   remainder__SCHL_Kindergarten  remainder__SCHL_Master's degree  \\\n",
       "0                           0.0                              0.0   \n",
       "1                           0.0                              0.0   \n",
       "2                           0.0                              1.0   \n",
       "3                           0.0                              0.0   \n",
       "4                           0.0                              0.0   \n",
       "\n",
       "   remainder__SCHL_No schooling completed  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "\n",
       "   remainder__SCHL_Nursery school, preschool  \\\n",
       "0                                        0.0   \n",
       "1                                        0.0   \n",
       "2                                        0.0   \n",
       "3                                        0.0   \n",
       "4                                        0.0   \n",
       "\n",
       "   remainder__SCHL_Professional degree beyond a bachelor's degree  \\\n",
       "0                                                0.0                \n",
       "1                                                0.0                \n",
       "2                                                0.0                \n",
       "3                                                0.0                \n",
       "4                                                0.0                \n",
       "\n",
       "   remainder__SCHL_Regular high school diploma  \\\n",
       "0                                          0.0   \n",
       "1                                          1.0   \n",
       "2                                          0.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "\n",
       "   remainder__SCHL_Some college, but less than 1 year  remainder__SEX_Female  \\\n",
       "0                                                0.0                     0.0   \n",
       "1                                                0.0                     0.0   \n",
       "2                                                0.0                     0.0   \n",
       "3                                                0.0                     0.0   \n",
       "4                                                0.0                     1.0   \n",
       "\n",
       "   remainder__SEX_Male  \n",
       "0                  1.0  \n",
       "1                  1.0  \n",
       "2                  1.0  \n",
       "3                  1.0  \n",
       "4                  0.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(features=ACSIncomeNew.features, definition_df=definition_df)\n",
    "# Here I convert categoricals into one-hot encoded (you don't have to, depending on what assumptions you use about the data)\n",
    "features, labels, groups = ACSIncomeNew.df_to_pandas(acs_data, categories=categories, dummies=True)\n",
    "features = features.fillna(-1) # Fill nulls with -1 which becomes necessary for the optimization\n",
    "########### Normalize continious features\n",
    "scaler = ColumnTransformer([(\"scaler\",StandardScaler(), [\"AGEP\", \"WKHP\", \"JWMNP\"])], remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "features_transformed = pd.DataFrame(\n",
    "    scaler.fit_transform(features), \n",
    "    columns=scaler.get_feature_names_out()\n",
    ")\n",
    "features_transformed.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting data into train-test. **Again, if you plan to use Cross-Validation then you should normalise features only inside of a fold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features_transformed.values, labels.values.reshape(-1), groups.values.reshape(-1), test_size=0.3, random_state=0, shuffle=True)\n",
    "\n",
    "# N = 1000 ### I am subsampling because it is slow on my machine\n",
    "# X_train = X_train[:N]\n",
    "# y_train = y_train[:N]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Regression model (without Fairness constraints)\n",
    "Let's first train a simple **Logistic Regression**. \n",
    "1. Use L2 penalty to train the model (you should find the optimal value for the regularizer)\n",
    "2. Calculate the total performance metric\n",
    "3. Calculate and compare the performance metric for each `SEX` group (use your favourite metric introduced during the course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.25867905,  0.16386303,  0.78042282, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.55527824,  0.16386303,  0.18596423, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.11655779,  0.16386303,  0.18596423, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 1.15993067,  1.70007775, -0.52738607, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.28654383,  1.70007775,  1.37488141, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 1.02556346, -0.60424432, -0.40849436, ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = ColumnTransformer([(\"scaler\", StandardScaler(), [\"AGEP\"])], remainder=\"passthrough\")\n",
    "# Create the pipeline\n",
    "logistic_model = Pipeline(steps=[\n",
    "    (\"logi\", LogisticRegression(max_iter=5000, penalty=\"l2\", C=0.8497534359086438, tol=1e-4, solver=\"saga\"))\n",
    "])\n",
    "# Fit the model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "log_y_pred = logistic_model.predict(X_test)\n",
    "log_y_prob = logistic_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7621132449937742"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bal_acc = balanced_accuracy_score(y_test, log_y_pred)\n",
    "bal_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Constrained Regression Model \n",
    "Now let's try to include the [Fairness Constraint](https://arxiv.org/abs/1706.02409)! You'll have to implement couple of things from scratch (as it is tricky to add a custom constraint function in `sklearn`.  To optimise the cost function let's use `scipy.optimize.fmin_tnc`. To calculate gradient you can use `fprime` attribute):\n",
    "1. Logistic Regression\n",
    "2. L2 penalisation\n",
    "3. **Individual** Fairness Constrained\n",
    "\n",
    "When you are finished with the implementation - you should evaluate performance on multiple choices of fairness weight, $\\lambda$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed breakdown\n",
    "The INDIVIDUAL constraint constraint looks like this:\n",
    "\n",
    "$$ \n",
    "f(\\beta,S) = \\left( \\frac{1}{n_1 n_2} \\sum_{(x_i,y_i)\\in S_1, (x_j,y_j)\\in S_2} d(y_i,y_j) (\\beta^T  \\textbf{x}_i - \\beta^T \\textbf{x}_j)^2  \\right) \n",
    "$$\n",
    "\n",
    "\n",
    "For the constrained optimization we have to solve a problem on the form:\n",
    "\n",
    "$$ \\min_\\beta \\left( \\ell (\\beta,S) + \\lambda f(\\beta,S)  +\\gamma \\Vert \\beta \\Vert_2 \\right) $$ \n",
    "\n",
    "where $\\ell$ is some loss function, $f$ is the constraint function, and the $\\gamma \\Vert \\beta \\Vert_2 $ is L2 regularization (we use it to avoid overfitting).\n",
    "(Basically we are minimizing the Lagrangian $\\mathscr{L} = \\ell (\\beta,S) + \\lambda f(\\beta,S)  +\\gamma \\Vert \\textbf{x} \\Vert_2$ with respect to $\\beta$ - in ML literature $\\mathscr{L}$ is often denoted as J)\n",
    "\n",
    "Because we are doing classification we are going to use logistic regression. The log loss function is:\n",
    "$$\n",
    "\\ell = \\frac{1}{m}\\sum_i^m\\left[ -y_i \\log(g(x_i)) - (1-y_i)\\log(1-g(x_i)) \\right], \\text{where } g(x_i) = \\frac{1}{1+\\exp(-\\beta_i x_i)}\n",
    "$$\n",
    "\n",
    "For the distance function we follow the approach from Berk et al. (2017) and set:\n",
    "$$d(y_i,y_j) = \\begin{cases}\n",
    "            1, &         \\text{if } y_i=y_j,\\\\\n",
    "            0, &         \\text{if } y_i\\neq y_j.\n",
    "    \\end{cases}$$\n",
    "    \n",
    "To minimize the total loss function we also need to estimate the gradient of $\\mathscr{L}$ with respect to $\\beta$. Here to update the $\\beta$ values we are just going the gradient's without the fairness constraing - this will make our lives considerably easier. The j'th element of the gradiend is defined as follows:\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\beta_j} \\approx \\frac{1}{m}\\left( \\sum_i  (g(x_i) - y_i) x[j] \\right)+ 2\\gamma \\beta_j\n",
    "$$\n",
    "\n",
    "##### A little clarification and tips:\n",
    "1. In order to simplify the exercise - we cut some corners. *Ideally* we should calculate the gradient in respect to the *individual fairness*. The gradient takes into the account only logistic and l2 loss (aka, parameters are updated based on those). At the same time, our *cost* has a *individual fairness* included. When the update of the parameters stops decreasing the cost, the `fmin_tnc` is going to stop optimisation. So our implementation is not entirely correct.\n",
    "2. In case you want to have a more correct implementation, you can do `opt.fmin_tnc(func=compute_cost, x0=betas, fprime = None, approx_grad= True, ...)`. It is quite long, but you can still do it\n",
    "3. I also suggest setting `ftol=1e-5`. \n",
    "4. Don not apply l2-regularization on the intercept (when you calculate the gradient).\n",
    "5. You should include $x_0 = 1$ in your data, for each observation (when it comes to the manual implementation of logistic regression) to include bias (i.e. weight $\\beta_0$).\n",
    "6. To keep the exercise simpler, let's fix $\\gamma = 1e-5$.\n",
    "7. Try $lambda$ is a range from around $1$ to $1e5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    This is logistic regression\n",
    "    f = 1/(1+exp(-beta^T * x))\n",
    "    This function assumes as input that you have already multiplied beta and X together\n",
    "    \"\"\"\n",
    "    NotImplemented\n",
    "\n",
    "def logistic_loss(y_true, y_pred, eps = 1e-10):\n",
    "    \"\"\"\n",
    "    Loss for the logistic regression, y_preds are probabilities\n",
    "    eps: epsilon for stability\n",
    "    \"\"\"\n",
    "    NotImplemented\n",
    "\n",
    "def l2_loss(beta):\n",
    "    \"\"\"\n",
    "    L2-Regularisation\n",
    "    \"\"\"\n",
    "    NotImplemented\n",
    "\n",
    "def fair_loss(y, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Group fairness Loss\n",
    "    \"\"\"\n",
    "    n = y.shape[0]\n",
    "    n1 = np.sum(groups == 1)\n",
    "    n2 = np.sum(groups == 2)\n",
    "    cost = 0\n",
    "    NotImplemented\n",
    "    return (cost/(n1*n2))\n",
    "\n",
    "def compute_gradient(beta,X,y, groups, _lambda,_gamma):\n",
    "    \"\"\"Calculate the gradient - used for finding the best beta values. \n",
    "       You do not need to use groups and lambda (fmin_tnc expects same input as in func, that's why they are included here)\"\"\"\n",
    "    grad = np.zeros(beta.shape)\n",
    "    NotImplemented\n",
    "\n",
    "    for i in range(len(grad)):\n",
    "        if i == 0: # we do not want to regularize the intercept\n",
    "            grad[i] =  ...\n",
    "        else:\n",
    "            grad[i] = ...\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def compute_cost(beta ,X,y, groups, _lambda, _gamma):\n",
    "    \"\"\"Computes cost function with constraints\"\"\"\n",
    "    NotImplemented\n",
    "    probs = sigmoid(X.dot(beta))\n",
    "    loss = logistic_loss(y, probs) + _lambda * fair_loss(y,X.dot(beta), groups) + _gamma * l2_loss(beta[1:])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cost with random beta-values and parameters\n",
    "compute_cost(\n",
    "    beta = np.random.rand(X_train.shape[1]),\n",
    "    X = X_train, \n",
    "    y = y_train,\n",
    "    groups = group_train, \n",
    "    _gamma = 1, \n",
    "    _lambda = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization with single `lambda` and `gamma` values\n",
    "### Set seed and define params\n",
    "np.random.seed(0)\n",
    "beta = np.random.rand(X_train.shape[1])\n",
    "lambda_ = # `1000` worked for robustly scaled continuous features\n",
    "gamma_ = # `1e-5` worked for robustly scaled continuous features\n",
    "\n",
    "### Run optimization\n",
    "result, _, _ = opt.fmin_tnc(\n",
    "    func=compute_cost,\n",
    "    x0=beta,\n",
    "    fprime=compute_gradient,\n",
    "    maxfun = 500,\n",
    "    args = (\n",
    "        X_train, \n",
    "        y_train,\n",
    "        group_train,\n",
    "        lambda_, \n",
    "        gamma_\n",
    "    ),\n",
    "    xtol=1e-7,\n",
    "    ftol=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define fairness constraint function\n",
    "def fairness_constraint(weights, X, y, sensitive_feature):\n",
    "    probs = 1 / (1 + np.exp(-X @ weights))  # Sigmoid function\n",
    "    group_0 = sensitive_feature == 0\n",
    "    group_1 = sensitive_feature == 1\n",
    "    fairness_gap = probs[group_0].mean() - probs[group_1].mean()\n",
    "    return fairness_gap\n",
    "\n",
    "# Gradient of fairness constraint\n",
    "def fairness_gradient(weights, X, y, sensitive_feature):\n",
    "    probs = 1 / (1 + np.exp(-X @ weights))\n",
    "    group_0 = sensitive_feature == 0\n",
    "    group_1 = sensitive_feature == 1\n",
    "    grad = np.mean(X[group_0] * probs[group_0][:, None], axis=0) - np.mean(X[group_1] * probs[group_1][:, None], axis=0)\n",
    "    return grad\n",
    "\n",
    "# Logistic loss function\n",
    "def logistic_loss(weights, X, y):\n",
    "    probs = 1 / (1 + np.exp(-X @ weights))\n",
    "    return -np.mean(y * np.log(probs) + (1 - y) * np.log(1 - probs))\n",
    "\n",
    "# Gradient of logistic loss\n",
    "def logistic_gradient(weights, X, y):\n",
    "    probs = 1 / (1 + np.exp(-X @ weights))\n",
    "    return np.dot(X.T, (probs - y)) / len(y)\n",
    "\n",
    "# Load and preprocess the data (assuming X, y, and sensitive feature are ready)\n",
    "X_train, X_test, y_train, y_test, s_train, s_test = train_test_split(X, y, sensitive_feature, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Optimize using constrained minimization\n",
    "initial_weights = np.zeros(X_train.shape[1])\n",
    "solution = opt.fmin_tnc(\n",
    "    func=logistic_loss, \n",
    "    x0=initial_weights, \n",
    "    fprime=logistic_gradient,\n",
    "    args=(X_train, y_train),\n",
    "    approx_grad=False,\n",
    "    constraints={'type': 'eq', 'fun': fairness_constraint, 'jac': fairness_gradient, 'args': (X_train, y_train, s_train)}\n",
    ")\n",
    "\n",
    "# Extract optimized weights\n",
    "optimal_weights = solution[0]\n",
    "\n",
    "# Evaluate fairness-optimized model\n",
    "pred_probs = 1 / (1 + np.exp(-X_test @ optimal_weights))\n",
    "pred_labels = (pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate fairness metrics\n",
    "group_0 = s_test == 0\n",
    "group_1 = s_test == 1\n",
    "fairness_difference = pred_probs[group_0].mean() - pred_probs[group_1].mean()\n",
    "print(\"Fairness Difference:\", fairness_difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Logistic regression function:\n",
    "    f = 1 / (1 + exp(-x))\n",
    "    Assumes input is already multiplied by beta.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logistic_loss(y_true, y_pred, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Computes the logistic loss.\n",
    "    Args:\n",
    "        y_true: Ground truth labels (0 or 1).\n",
    "        y_pred: Predicted probabilities.\n",
    "        eps: Small value for numerical stability.\n",
    "    \"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)  # Prevent log(0) errors\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def l2_loss(beta):\n",
    "    \"\"\"\n",
    "    L2 Regularization loss.\n",
    "    Args:\n",
    "        beta: Model coefficients (excluding intercept).\n",
    "    \"\"\"\n",
    "    return np.sum(beta ** 2)\n",
    "\n",
    "def fair_loss(y, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Computes group fairness loss based on demographic parity.\n",
    "    Ensures similar predictions across groups.\n",
    "    Args:\n",
    "        y: True labels.\n",
    "        y_pred: Predicted probabilities.\n",
    "        groups: Group membership (e.g., 1 for males, 2 for females).\n",
    "    \"\"\"\n",
    "    n1 = np.sum(groups == 1)\n",
    "    n2 = np.sum(groups == 2)\n",
    "\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return 0  # Avoid division by zero if a group is missing\n",
    "\n",
    "    group1_mean = np.mean(y_pred[groups == 1])\n",
    "    group2_mean = np.mean(y_pred[groups == 2])\n",
    "\n",
    "    return (group1_mean - group2_mean) ** 2 / (n1 * n2)\n",
    "\n",
    "def compute_gradient(beta, X, y, groups, _lambda, _gamma):\n",
    "    \"\"\"\n",
    "    Computes gradient for optimizing logistic regression with fairness constraint.\n",
    "    Args:\n",
    "        beta: Model coefficients.\n",
    "        X: Feature matrix.\n",
    "        y: Target labels.\n",
    "        groups: Group membership.\n",
    "        _lambda: Fairness regularization parameter.\n",
    "        _gamma: L2 regularization parameter.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    y_pred = sigmoid(X.dot(beta))\n",
    "\n",
    "    grad = np.zeros_like(beta)\n",
    "    error = y_pred - y\n",
    "\n",
    "    grad = X.T.dot(error) / n  # Gradient of logistic loss\n",
    "\n",
    "    # Fairness gradient\n",
    "    fair_grad = np.zeros_like(beta)\n",
    "    n1, n2 = np.sum(groups == 1), np.sum(groups == 2)\n",
    "\n",
    "    if n1 > 0 and n2 > 0:\n",
    "        group1_idx = (groups == 1)\n",
    "        group2_idx = (groups == 2)\n",
    "\n",
    "        fair_grad = (np.mean(X[group1_idx], axis=0) - np.mean(X[group2_idx], axis=0)) / (n1 * n2)\n",
    "\n",
    "    # L2 regularization (excluding intercept)\n",
    "    grad[1:] += _gamma * beta[1:]\n",
    "\n",
    "    # Add fairness constraint gradient\n",
    "    grad += _lambda * fair_grad\n",
    "\n",
    "    return grad\n",
    "\n",
    "def compute_cost(beta, X, y, groups, _lambda, _gamma):\n",
    "    \"\"\"\n",
    "    Computes cost function with fairness and regularization constraints.\n",
    "    Args:\n",
    "        beta: Model coefficients.\n",
    "        X: Feature matrix.\n",
    "        y: Target labels.\n",
    "        groups: Group membership.\n",
    "        _lambda: Fairness regularization parameter.\n",
    "        _gamma: L2 regularization parameter.\n",
    "    \"\"\"\n",
    "    probs = sigmoid(X.dot(beta))\n",
    "    loss = logistic_loss(y, probs)\n",
    "    \n",
    "    # Add fairness loss and L2 regularization\n",
    "    loss += _lambda * fair_loss(y, probs, groups) + _gamma * l2_loss(beta[1:])\n",
    "    \n",
    "    return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
